{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Ifely6MXGivf"
      },
      "outputs": [],
      "source": [
        "# Notebook-friendly HigherEdJobs Wayback scraper (no argparse)\n",
        "# pip install -q requests beautifulsoup4 lxml\n",
        "\n",
        "from __future__ import annotations\n",
        "import csv, json, re, time\n",
        "from dataclasses import dataclass, asdict\n",
        "from datetime import datetime\n",
        "from typing import Dict, Iterable, List, Optional, Tuple\n",
        "from urllib.parse import urlsplit, parse_qs\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "CDX_ENDPOINT = \"https://web.archive.org/cdx/search/cdx\"\n",
        "WAYBACK_PREFIX = \"https://web.archive.org/web\"\n",
        "\n",
        "DETAIL_PATTERNS = [\n",
        "    \"https://www.higheredjobs.com/details.cfm\",\n",
        "    \"https://www.higheredjobs.com/faculty/details.cfm\",\n",
        "    \"https://www.higheredjobs.com/admin/details.cfm\",\n",
        "    \"https://www.higheredjobs.com/executive/details.cfm\",\n",
        "    \"http://www.higheredjobs.com/details.cfm\",\n",
        "    \"http://www.higheredjobs.com/faculty/details.cfm\",\n",
        "    \"http://www.higheredjobs.com/admin/details.cfm\",\n",
        "    \"http://www.higheredjobs.com/executive/details.cfm\",\n",
        "]\n",
        "\n",
        "HEADERS = {\n",
        "    \"User-Agent\": \"MonkieResearch/1.0 (+https://monkie.ai; research use; contact: you@example.com)\"\n",
        "}\n",
        "REQUEST_DELAY_SEC = 0.25  # be polite\n",
        "\n",
        "@dataclass\n",
        "class JobPosting:\n",
        "    job_id: Optional[str]\n",
        "    original_url: str\n",
        "    wayback_timestamp: str\n",
        "    wayback_url: str\n",
        "    category: Optional[str]\n",
        "    title: Optional[str]\n",
        "    institution: Optional[str]\n",
        "    location: Optional[str]\n",
        "    posted_date_str: Optional[str]\n",
        "    description_text: Optional[str]\n",
        "\n",
        "def parse_iso_date(date_str: str) -> str:\n",
        "    for fmt in (\"%Y-%m-%d\", \"%Y/%m/%d\", \"%Y%m%d\", \"%Y-%m\", \"%Y%m\", \"%Y\"):\n",
        "        try:\n",
        "            dt = datetime.strptime(date_str, fmt)\n",
        "            break\n",
        "        except ValueError:\n",
        "            continue\n",
        "    else:\n",
        "        raise ValueError(f\"Cannot parse date: {date_str}\")\n",
        "    if len(date_str) in (4, 6, 7):\n",
        "        if len(date_str) == 4:\n",
        "            dt = dt.replace(month=1, day=1)\n",
        "        else:\n",
        "            dt = dt.replace(day=1)\n",
        "    return dt.strftime(\"%Y%m%d\")\n",
        "\n",
        "def cdx_query(session: requests.Session, url_prefix: str, from_ymd: str, to_ymd: str,\n",
        "              limit: int = 2000, offset: int = 0) -> List[Dict[str, str]]:\n",
        "    params = {\n",
        "        \"url\": url_prefix,\n",
        "        \"matchType\": \"prefix\",\n",
        "        \"from\": from_ymd,\n",
        "        \"to\": to_ymd,\n",
        "        \"output\": \"json\",\n",
        "        \"fl\": \"timestamp,original,mimetype,statuscode,digest,length\",\n",
        "        \"filter\": [\"statuscode:200\", \"mimetype:text/html\"],\n",
        "        \"limit\": str(limit),\n",
        "        \"offset\": str(offset),\n",
        "    }\n",
        "    resp = session.get(CDX_ENDPOINT, params=params, headers=HEADERS, timeout=30)\n",
        "    resp.raise_for_status()\n",
        "    data = resp.json()\n",
        "    if not data:\n",
        "        return []\n",
        "    header, *rows = data\n",
        "    out = []\n",
        "    for row in rows:\n",
        "        if len(row) == len(header):\n",
        "            out.append(dict(zip(header, row)))\n",
        "    return out\n",
        "\n",
        "def iter_all_cdx(session: requests.Session, url_prefix: str, from_ymd: str, to_ymd: str,\n",
        "                 max_records: Optional[int]) -> Iterable[Dict[str, str]]:\n",
        "    fetched, limit, offset = 0, 2000, 0\n",
        "    while True:\n",
        "        if max_records is not None and fetched >= max_records:\n",
        "            return\n",
        "        chunk = cdx_query(session, url_prefix, from_ymd, to_ymd, limit=limit, offset=offset)\n",
        "        if not chunk:\n",
        "            return\n",
        "        for row in chunk:\n",
        "            yield row\n",
        "            fetched += 1\n",
        "            if max_records is not None and fetched >= max_records:\n",
        "                return\n",
        "        offset += limit\n",
        "        time.sleep(REQUEST_DELAY_SEC)\n",
        "\n",
        "def build_wayback_url(timestamp: str, original: str) -> str:\n",
        "    return f\"{WAYBACK_PREFIX}/{timestamp}id_/{original}\"\n",
        "\n",
        "def categorize_from_path(original_url: str) -> Optional[str]:\n",
        "    path = urlsplit(original_url).path.lower()\n",
        "    if \"/faculty/\" in path: return \"faculty\"\n",
        "    if \"/admin/\" in path: return \"administrative\"\n",
        "    if \"/executive/\" in path: return \"executive\"\n",
        "    return \"other\"\n",
        "\n",
        "def extract_jobcode(original_url: str) -> Optional[str]:\n",
        "    qs = parse_qs(urlsplit(original_url).query)\n",
        "    for key in (\"JobCode\", \"jobcode\", \"JobID\", \"jobid\"):\n",
        "        if key in qs and qs[key]:\n",
        "            return qs[key][0]\n",
        "    return None\n",
        "\n",
        "def clean(txt: Optional[str]) -> Optional[str]:\n",
        "    if not txt: return None\n",
        "    return re.sub(r\"\\s+\", \" \", txt).strip()\n",
        "\n",
        "def parse_job_html(html: str) -> Tuple[Optional[str], Optional[str], Optional[str], Optional[str]]:\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "    title = None\n",
        "    h1 = soup.find(\"h1\")\n",
        "    if h1 and clean(h1.get_text()): title = clean(h1.get_text())\n",
        "    if not title:\n",
        "        og = soup.find(\"meta\", attrs={\"property\": \"og:title\"})\n",
        "        if og and og.get(\"content\"): title = clean(og[\"content\"])\n",
        "    if not title and soup.title: title = clean(soup.title.get_text())\n",
        "\n",
        "    def find_label_value(labels: List[str]) -> Optional[str]:\n",
        "        label_regex = re.compile(r\"^\\s*(%s)\\s*[:：]\\s*$\" % \"|\".join(map(re.escape, labels)), re.I)\n",
        "        for tag in soup.find_all(string=label_regex):\n",
        "            parent = tag.parent\n",
        "            if parent and parent.next_sibling:\n",
        "                try:\n",
        "                    candidate = clean(getattr(parent.next_sibling, \"get_text\", lambda: str(parent.next_sibling))())\n",
        "                    if candidate: return candidate\n",
        "                except Exception:\n",
        "                    pass\n",
        "            line = clean(parent.get_text()) if parent else None\n",
        "            if line:\n",
        "                value = re.sub(label_regex, \"\", line, count=1).strip(\" :-\")\n",
        "                if value: return value\n",
        "        return None\n",
        "\n",
        "    institution = find_label_value([\"Institution\", \"College/University\", \"Employer\", \"Organization\"])\n",
        "    location = find_label_value([\"Location\", \"City/State\", \"City\", \"State\"])\n",
        "    posted_date_str = find_label_value([\"Posted\", \"Posted On\", \"Date Posted\", \"Posting Date\"])\n",
        "\n",
        "    if not institution and title:\n",
        "        m = re.search(r\"\\bat\\s+([^|–—-]+)$\", title)\n",
        "        if m: institution = clean(m.group(1))\n",
        "\n",
        "    return title, institution, location, posted_date_str\n",
        "\n",
        "def scrape_higheredjobs(date_from: str,\n",
        "                        date_to: str,\n",
        "                        out_csv: str = \"higheredjobs_wayback.csv\",\n",
        "                        out_ndjson: str = \"higheredjobs_wayback.ndjson\",\n",
        "                        max_per_pattern: Optional[int] = None,\n",
        "                        max_pages: Optional[int] = None,\n",
        "                        timeout: int = 30) -> Dict[str, JobPosting]:\n",
        "\n",
        "    from_ymd = parse_iso_date(date_from)\n",
        "    to_ymd = parse_iso_date(date_to)\n",
        "    sess = requests.Session(); sess.headers.update(HEADERS)\n",
        "\n",
        "    seen_jobs: Dict[str, JobPosting] = {}\n",
        "    total_fetched = 0\n",
        "\n",
        "    for url_prefix in DETAIL_PATTERNS:\n",
        "        print(f\"[CDX] {url_prefix}  [{from_ymd}..{to_ymd}]\")\n",
        "        for row in iter_all_cdx(sess, url_prefix, from_ymd, to_ymd, max_per_pattern):\n",
        "            ts, original = row.get(\"timestamp\"), row.get(\"original\")\n",
        "            if not ts or not original: continue\n",
        "\n",
        "            wayback_url = build_wayback_url(ts, original)\n",
        "            job_id = extract_jobcode(original) or f\"NOJOBCODE-{hash(original) & 0xFFFFFFFF:x}\"\n",
        "            if job_id in seen_jobs: continue  # keep earliest\n",
        "\n",
        "            try:\n",
        "                time.sleep(REQUEST_DELAY_SEC)\n",
        "                r = sess.get(wayback_url, timeout=timeout)\n",
        "                if r.status_code != 200: continue\n",
        "                html = r.text\n",
        "            except requests.RequestException:\n",
        "                continue\n",
        "\n",
        "            if \"Page cannot be displayed due to robots.txt\" in html or \\\n",
        "               \"Wayback Machine doesn't have that page archived\" in html:\n",
        "                continue\n",
        "\n",
        "            title, institution, location, posted_date_str = parse_job_html(html)\n",
        "\n",
        "            soup = BeautifulSoup(html, \"html.parser\")\n",
        "            candidates = soup.find_all([\"div\", \"section\", \"article\"], limit=50)\n",
        "            best = \"\"\n",
        "            for block in candidates:\n",
        "                txt = clean(block.get_text(\" \"))\n",
        "                if txt and len(txt) > len(best): best = txt\n",
        "            description_text = best if best else clean(soup.get_text(\" \"))\n",
        "\n",
        "            jp = JobPosting(\n",
        "                job_id=job_id,\n",
        "                original_url=original,\n",
        "                wayback_timestamp=datetime.strptime(ts, \"%Y%m%d%H%M%S\").isoformat(),\n",
        "                wayback_url=wayback_url,\n",
        "                category=categorize_from_path(original),\n",
        "                title=title,\n",
        "                institution=institution,\n",
        "                location=location,\n",
        "                posted_date_str=posted_date_str,\n",
        "                description_text=description_text,\n",
        "            )\n",
        "            seen_jobs[job_id] = jp\n",
        "            total_fetched += 1\n",
        "\n",
        "            if max_pages is not None and total_fetched >= max_pages:\n",
        "                break\n",
        "        if max_pages is not None and total_fetched >= max_pages:\n",
        "            break\n",
        "\n",
        "    print(f\"[DONE] Unique jobs collected: {len(seen_jobs)}\")\n",
        "\n",
        "    with open(out_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        w = csv.writer(f)\n",
        "        w.writerow([\n",
        "            \"job_id\",\"category\",\"title\",\"institution\",\"location\",\"posted_date_str\",\n",
        "            \"wayback_timestamp\",\"original_url\",\"wayback_url\",\"description_text\"\n",
        "        ])\n",
        "        for jp in seen_jobs.values():\n",
        "            w.writerow([\n",
        "                jp.job_id, jp.category or \"\", jp.title or \"\", jp.institution or \"\",\n",
        "                jp.location or \"\", jp.posted_date_str or \"\", jp.wayback_timestamp,\n",
        "                jp.original_url, jp.wayback_url, (jp.description_text or \"\")[:20000]\n",
        "            ])\n",
        "\n",
        "    with open(out_ndjson, \"w\", encoding=\"utf-8\") as f:\n",
        "        for jp in seen_jobs.values():\n",
        "            f.write(json.dumps(asdict(jp), ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "    print(f\"Wrote: {out_csv} and {out_ndjson}\")\n",
        "    return seen_jobs\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_ = scrape_higheredjobs(\"2023-07-01\", \"2023-8-30\", max_pages=200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oaq-e74KGxeN",
        "outputId": "f847348c-1d5c-4b63-dae1-4c15c3eff7d2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CDX] https://www.higheredjobs.com/details.cfm  [20230701..20230830]\n",
            "[DONE] Unique jobs collected: 200\n",
            "Wrote: higheredjobs_wayback.csv and higheredjobs_wayback.ndjson\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m-wOSVZuHa9q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}